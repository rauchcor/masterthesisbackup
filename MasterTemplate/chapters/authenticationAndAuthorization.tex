%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work to Identity Management}\label{chap:authenticationandauthorization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapterstart



\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{images/on-the-internet-peter-steine}
	\caption{"On the Internet, nobody knows you are a dog"(\cite{Steiner:Dog:1992})}
	\label{fig:on-the-internet-peter-steine}
\end{figure}

The cartoon of (\cite{Steiner:Dog:1992}) in figure~\ref{fig:on-the-internet-peter-steine}, showing two dogs sitting in front of a computer with the iconic title cited above became an illustration of how people view anonymity on the Internet. Being anonymous on the Internet, however, is not that easy anymore. The use of mobile devices has changed the way how we access information, interact with each other and share content. Digital services are used to access information, which is opening fraught opportunities to attackers who are trying to impersonate someone and access confidential information.  With this change of user behavior, the way we think of authentication and authorization methods has to adjust. Identity management is mandatory to provide a seamless user experience, including identity proofing, authentication, and assertions in federated environments [cf. (\cite{NIST:2017:DIG}), (\cite{Corre:2017:WHI})].


Users find themselves struggling using multiple devices, accounts, and services. The user's burden of this site-by-site account management is putting security at risk. The goal of new authentication and authorization solutions is to help the user managing his accounts by providing single sign-on (SSO), based on an exchange of identity-related assertion across security domains in a scalable way [cf. (\cite{Corre:2017:WHI})].  

In this chapter, existing work and methods to authenticate and authorize users are described. The part is split up in the chapters Security Considerations, Identity Proofing, Authentication, Assertion, Token-based Authentication and SSO Federation Systems.


\section{Security Considerations}
\label{securityConsiderations}

Before getting further into the topic of authentication and authorization, this section will shed light on some basic security principles, concerning authentication and authorization, which help to understand the need for authentication and authorization mechanisms. 

Some basic design principles formulated by Saltzer \& Schroeder (1975) were paraphrased by Neumann (2013) and are still relevant today. The principles give an underlying overview of what should be the focus when designing a secure system. The first of the ten basic security principles formulated by  Saltzer \& Schroeder (1975) is the economy of mechanism which means to keep the design as simple as possible. The next principle describes that access should not be explicitly denied; instead it should be explicitly permitted. For example, when using Access Control Lists (ACLs), all access should be explicitly denied by default. This kind of access control is called Fail-safe defaults.
Furthermore,  Saltzer \& Schroeder (1975) states the complete mediation principle, which states that every access to every object has to be checked for authority without exceptions. A fundamental concept that is part of the basic security principles is open design. The design of an application should not be secret. It can not be assumed that design secrecy will enhance security. This principle is often applied in cryptography. For example, the design of cryptographic algorithms is available for the public, and just the keys remain secret. A broadly used principle in authentication is the separation of privileges. Separation of privileges means that two keys should be used to protect resources if feasible and privileges should be separated.
Every application and user should be provided with least privileges they need to complete their job. The existence of overly powerful mechanisms such as superuser is inherently dangerous - this is called least privilege. The least common mechanism principles compel to minimize the number of mechanisms that common to more than one user are used by all users. In authentication users often get frustrated with the complicated sign-in processes. Therefore, the psychological acceptability should be kept in mind. Keep it simple. The design of the interface for the user should be easy to understand so that the user routinely and automatically applies the protection mechanism correctly. The attack factor or work factor is essential to protect sensitive resources. Cost-to-protect should commensurate with threats and expected risks. It should not be possible to circumvent the mechanism with the resources of the attacker.   
The last one is a recording of compromises which means to provide trails of evidence which are tamper-proof and difficult to bypass. All of these principles are important when choosing an authentication system and need to be considered carefully [cf. (\cite{Neumann:2018:PTC}), (\cite{Saltzer:PICS})]. 

Besides formulating these fundamental principles which will be discussed in various forms,  Saltzer \& Schroeder (1975) also discuss the terms "privacy" and "security". Those terms get frequently used by authors writing about information storing systems, like in this paper. However, the terms "privacy" and "security" are often used very differently.  Saltzer \& Schroeder (1975) for example, defines "privacy" as the ability of an individual to specify whether, when and to whom sensible information is released, and "security" is described as a technique that can control who can modify resources on a computer. Another more recent description of the terms "security" and "privacy" is defined by Brooks et al. (2017). Brooks et al. (2017), states the importance of the distinction between privacy and security. This distinction between privacy and security are essential because there are security issues unrelated to privacy, just as there are privacy issues that are unrelated to security. While security concerns arise from illegal system behavior, privacy concerns arise from byproducts of authorized personally identifiable information (PII) processing. Even byproducts that are considered to protect PII can raise security concerns, for example, it can be questioned to which degree a tool for persistent activity monitoring should reveal information about individuals that are related to security purposes. However, security and privacy have in common that they want to protect personal information and resources or PII [cf. (\cite{Brooks:2017:IPE}), (\cite{Saltzer:PICS})].

These security issues and privacy issues, of course, raise particular concerns for users as well as for companies offering authentication services. When it comes to protecting personal resources, there are three primary concerns. According to Todorov (2007), those three concerns are confidentiality, integrity, and availability. The term confidentiality means that personal information is protected from disclosure to unauthorized individuals and organizations. Integrity or integrity of information, is protecting information from accidental or intentional tampering. Modification of confidential data may affect the data validity. Availability is the need to be able to access information at the time a user requests it. The availability of the services that exposes information has to be given. In an ideal world companies offering authentication and authorization services will do everything to use the best technologies regarding countermeasures to protect confidentiality, integrity, and availability. Establishing countermeasures, however, can be costly leading to a trade-off between costs and level of production of information. A typical approach to establishing information security management is to analyze risks first and then from counter measurements [cf. (\cite{Todorov:2007:MUI})]. 

The paper 'Digital Identity Guidelines' by Grassi, Garcia \& L. (2017), splits the risk assessment into different sections, rather than combining identity proofing, authentication, and federation of digital services into a single Level of Assurance (LOA). In order to provide the most effective approach each of the section is analyzed regarding risk and impacts of failure. The assessment should help to choose security controls and mitigation strategies that help to avoid errors. Errors that can be avoided with an extensive risk assessment among others are identity proofing, authentication and federation errors. Identity proofing risks include a malicious applicant successfully poses as someone else or for example the impacts of collecting more information about an applicant then required. Choosing the right level of assurance in the identity proofing process gives robustness and confidence to the determination of identity. The right authentication assurance level is chosen to mitigate problems in the authentication process, and the binding between an authenticator and a specific individual. An example of a potential error, is the illegal use of someone else's credentials by an applicant. Last but not least, the accurate federation assessment helps to mitigate potential federation errors. Federation errors might occur when an identity assertion is compromised. Analyzing these risks makes it easier to choose an assurance level for each category. Accordingly, the risk gets categorized of harm according to the paper Digital Identity Guidelines by Grassi, Garcia \& L. (2017): 

\begin{itemize}
	\item Inconvenience, distress, or damage to standing or reputation
	\item Financial loss or agency liability
	\item Harm to agency programs or public interests
	\item Personal safety
	\item Civil or criminal violations
\end{itemize}

Each of the categories can then be rated with a potential impact level which can be either low, moderate or high. The result of this risk assessment can then be the basis for choosing the appropriate identity, authentication and federation assurance level. The assurance level determination is just relevant for analyzing the risk of online transaction offered by digital services and should not be used for the complete business process which may include offline processing in an entirely segmented system [cf. (\cite{NIST:2017:DIG})]. 

The result of all design principles, security and privacy considerations should be a system, network or component that is trustworthy. According to Neumann (2018), trustworthiness is given if an entity satisfies its specified requirement, after a reliable assessment. The requirements that deserve special consideration are, those that are critical to an enterprise, mission, system, network or other entity. One of the requirements to make a system trustworthy is a reliable authentication and authorization process. These processes are discussed in more detail in the next sections [cf. (\cite{Neumann:2018:PTC})].



\section{Identity Proofing}
\label{identityProofing}

“Digital identity is the unique representation of a subject engaged in an online transaction. The process used to verify a subject’s association with their real-world identity is called identity proofing” (\cite{NIST:2017:DIG})

A digital identity as explained above is the result of what is called the authentication process. It is a way of identifying the user as who they claim to be. In order to prove the user's identity presentation, validation and verification of a minimum set of attributes are necessary [cf. (\cite{Boyd:2012:GSOAuth})]. 

To become authenticated, the user has to go through certain steps on the way. These steps are enabled by typical components unique to the authentication process. Todorov (2007) identifies three typical components that are part of the authentication of the Supplicant, the Authenticator, and the Security Database. The Supplicant is the party that provides the evidence to prove the identity of a user or client. The result of the authentication process should be the authenticated user or client. The Authenticator, is responsible for ascertaining the user identity. After proving the identity, the authenticator can authorize or audit the user access to resources. The Security Authority Database is a storage or a mechanism to check the user's credentials. The storage can be represented by as much as a flat file, a server on the network providing centralized user authentication or a distributed authentication server [cf. (\cite{Todorov:2007:MUI})]. 



\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/identity_proofing_architectur}
	\caption[Componets Authentication]{Components of User Authentication System (\cite{Todorov:2007:MUI})}
	\label{fig:componentsuserauthenticationsystem}
\end{figure}

It is vital that all the components as shown in figure~\ref{fig:componentsuserauthenticationsystem}, of a user authentication system can communicate independently of each other. Whether or not all communication channels are used, depends on the authentication mechanism and the model of trust that it implements. For example, the Kerberos authentication protocol does not feature direct communication between the authenticator and security server [cf. (\cite{Todorov:2007:MUI})]. 

The paper 'Digital Identity Guidelines - Enrollment and Identity Proofing' by Grassi, J.L. Fenton, et al. (2017) also describes components that are similar to the ones described by Todorov (2007) in figure~\ref{fig:componentsuserauthenticationsystem}. Furthermore the paper 'Digital Identity Guidelines - Enrollment and Identity Proofing' by Grassi, J.L. Fenton, et al. (2017) describes three different identity assurance level shown in table~\ref{tab:ial}. These assurance level state to which degree identity assurance can or should be provided in a system [cf. (\cite{NIST:2017:DIGEIP}), (\cite{Todorov:2007:MUI})]. 


\begin{table}[h]
	\centering
	\begingroup
	\setlength{\tabcolsep}{10pt} % Default value: 6pt
	\renewcommand{\arraystretch}{1.5} % 
	\begin{tabular}{cl}
		\hline
		\multicolumn{2}{c}{\cellcolor[HTML]{656565}{\color[HTML]{FFFFFF} Identity Assurance Level}}                                                                                                                                                                   \\ \hline
		\multicolumn{1}{|c|}{\textbf{IAL1}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}There is no need to link the applicant to a real-life identity.   All\\ attributes provided are self asserted and are neither validated nor \\ verified.\end{tabular}}   \\ \hline
		\multicolumn{1}{|c|}{\textbf{IAL2}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}The evidence provided supports the real-world existence of the \\ claimed identity. The identity proofing has to be either  remote \\ or physical present.\end{tabular}} \\ \hline
		\multicolumn{1}{|c|}{\textbf{IAL3}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}The identity proofing requires physical presence. Attributes must\\ be verified by a authorized and trained CSP.\end{tabular}}                                            \\ \hline
	\end{tabular}
	\endgroup
	\caption{Identity Assurance Levels (\cite{NIST:2017:DIG}, p.18)} \label{tab:ial}
\end{table}
\pagebreak[4]
Moreover, the paper 'Digital Identity Guidelines - Enrollment and Identity Proofing' by Grassi, J. L. Fenton, et al. (2017), describes additional expectations of the identity proofing process. The expected outcome of the identity proofing process should be that the claimed identity is linked to a single, unique identity. The provided evidence for the claimed identity has to be validated. The validation should prove that the evidence is correct, genuine and exists in the real world.
This section describes certain aspects of the authentication process and gives a very general description of components that can be part of an authentication process. Furthermore, identity proofing assurance level is provided that are relevant for the risk assessment. This chapter describes that the user has to provide proof of the claimed identity but lacks to describes how this provided proof might look and how it is validated – this is described in the next section~\ref{authentication} [cf. (\cite{NIST:2017:DIGEIP})].
	

\section{Authentication}
\label{authentication}
For a very typical authentication process, the user provides its username and password when the application demands it. If the user provides a correct username and password, an application assumes that the user is indeed the owner of the account. The evidence provided by the user in the authentication process is called credentials. Most of the time, as mentioned above, credentials get provided in the form of username and password. Nevertheless, credentials may as well take other forms like Personal Identification Numbers (PIN), key cards, eye scanners and so on [cf. (\cite{Todorov:2007:MUI}), (\cite{Boyd:2012:GSOAuth})]. 

Credentials, which prove the identity of an entity and find use as authenticators in authentication systems, are called factors. The 'Digital Identity Guidelines' by Grassi, Garcia \& L. (2017) categorizes the following types of factors:

\begin{itemize}  
	\item Something, the user, knows - Cognitive information the user has to remember. Examples include passwords, PIN, answers to secret questions.
	\item What the user has - something the user owns. Examples include a security token, driving license, one-time password (OTP). 
	\item What the user is - biometric information of the user. Examples include fingerprint, voice, and face. 
\end{itemize}

Other types of information which are not considered authentication factors but  can be used to enrich the authentication process according to Dasgupta, Arunava \& Abhijit (2017) are:

\begin{itemize}
	\item Where the user is - the location of the user can is used as a fourth factor of authentication. Examples include GPS, IP addresses.
	\item When the user logs on - Time can also be extracted as a separate factor. Verification of employee’s identification in different office hours can prevent many kinds of grave data breaches. The time factor can easily prevent online banking fraud events to a great extent. 
\end{itemize}

The authenticators are based on secrets that can be either public key pairs (asymmetric keys) or shared secrets (symmetric keys). Public key pairs consist of a private and a public key. The private key gets stored on the authenticator. The holder of the private key can use it to prove they control the authenticator. The verifier of the authenticator can then use the public key - most likely received with the help of a public key certificate - together with the authentication protocol to verify the identity of the user. Shared secrets can be either symmetric keys or memorized secrets. Both keys and passwords can be used with similar protocols. However, there is a huge difference for the user. Symmetric keys are generally stored on hardware; secrets have to be memorized by the subscriber which can lead to multiple vulnerabilities. Cryptographic keys are typically long enough to make network-based guessing attacks untenable but if the user chooses short, memorable passwords the system may be vulnerable [cf. (\cite{NIST:2017:DIG}), (\cite{Dasgupta:2017:AUA})]. 


To secure a solution properly, it should at least use two factors of the three listed above. To make use of more than one factor of a pool of potential credentials to verify the identity of a user is referred to as Multi-factor Authentication (MFA). The goal of multi-factor authentication is to provide a layered defense and make it harder for unauthorized individuals to gain access. If one of the factors breaks, the service can still rely on the non-compromised authentication factors [cf. (\cite{Dasgupta:2017:AUA})].

Using just one factor is called Single Factor Authentication(SFA). Dasgupta, Arunava \& Abhijit (2017) clearly describes the drawbacks SFA has compared to MFA, primarily the universal used password-based authentication. The user needs to remember different passwords for multiple accounts. Therefore, the user often reuses one password also known as password fatigue [cf. (\cite{Dasgupta:2017:AUA})].

In an Interview by Tomkins (2009) with Jon Brody, he explains Password Fatigue like the following. An average user has 15 accounts; some people might even have up to 30 accounts - far too much to manage appropriately. Users then tend to adopt specific password patterns like using simple passwords for nontransactional sites and complex passwords for banking sites. Since many complex passwords are hard to remember users also often reuse passwords for different services at one point - this is called password fatigue [cf. (\cite{Tomkins:2009:DPF})]. 

Besides password fatigue Todorov (2007) draws attention to one of the significant challenges of secure user authentication represented by default passwords. Vendors often ship their devices with pre-configured standard passwords. Although vendors recommend changing default passwords, system architects and engineers often fail to do so because they are more focused on the business logic than on security causing security issues. Systems with default passwords are more straightforward to attack, for example by knowing or guessing the device the attacker has an easy time authenticating with the system accordingly [cf. (\cite{Todorov:2007:MUI})]. 


Due to the problem of password fatigue and default passwords, one factor like a password might easily gets compromised. The user can then no longer use the service until the repair of the system which can lead to a delay of the user when trying to access necessary information. Also, there is a risk that the user does not notice the compromise of the single factor which can lead to devastating effects [cf. (\cite{Dasgupta:2017:AUA})]. 


\begin{table}[h]
	\centering
	\begingroup
	\setlength{\tabcolsep}{10pt} % Default value: 6pt
	\renewcommand{\arraystretch}{1.5} % 
	\begin{tabular}{cl}
		\hline
		\multicolumn{2}{c}{\cellcolor[HTML]{656565}{\color[HTML]{FFFFFF} Authenticator Assurance Level}}                                                                                                                                                                                                                                                                                                                                     \\ \hline
		\multicolumn{1}{|c|}{\textbf{AAL1}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}The first level defines that a claimant has to provide some kind of\\ assurance that they control an authenticator bound to a subscriber's\\ account through a secure authentication protocol. The guarantee can be\\ either in the form of a single-factor or multi-factor authentication.\end{tabular}}                                         \\ \hline
		\multicolumn{1}{|c|}{\textbf{AAL2}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}The second level requires the use of at least two distinct authentication\\ factors. The confidence that the claimant controls the authenticators of\\ the subscriber's account is high at this level. Appropriate cryptographic\\ techniques are required.\end{tabular}}                                                                     \\ \hline
		\multicolumn{1}{|c|}{\textbf{AAL3}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}The third level provides a very high level of confidence that the claimant\\ is in possession of the authenticators bound to the subscriber. Claimants\\ need to prove possession of the key with two distinct authenticators\\ through a secure authentication protocol. One of the authenticators needs\\ to be hardware-based.\end{tabular}} \\ \hline
	\end{tabular}
	\endgroup
	\caption{Authenticator Assurance Levels (\cite{NIST:2017:DIG}, p.19)} \label{tab:aal}
\end{table}
\pagebreak[4]
When designing an authentication process by using multiple factors the designers of the process should be acutely aware of the type of application and the information that has to be secured. For example, a solution for an international bank should have different standards than an application for a maintaining a grocery list. On the one hand, challenging and complex authentication processes for trivial applications might scare away users. On the other hand, simple methods for applications protecting sensitive data might drive users away as well. Application designers should always try to find a middle way that suits both parties, the application owners and the users [cf. (\cite{NIST:2017:DIG})]. 


Furthermore the paper 'Digital Identity Guidelines - Authentication and Lifecycle Management' by Grassi, J. Fenton, et al. (2017) defines three different authenticator assurance levels to make sure the system a company is designing uses the appropriate authenticators for their purpose. Each of the authenticator assurance levels has to fulfill all the requirements of their successor. The paper 'Digital Identity Guidelines - Authentication and Lifecycle Management' by  Grassi, J. Fenton, et al. (2017) describes three levels shown in table~\ref{tab:aal} [cf. (\cite{NIST:2017:DIGAL})].

Authentication is a critical part of every application. As users are getting more concerned on security, the pressure on developers grows to provide a solution that secures sensitive data while keeping up usability standards, which can often be a trade-off. Complex applications need complex security which can mean high costs for individually developed solutions; therefore application developers should also think about using federated authentication solutions [cf. (\cite{NIST:2017:DIGAL})].


\section{Assertion}
\label{assertion}
The authentication and authorization process are very closely related to each other and for users often hard to separate. After the authentication process of a user, the application has now proved that the user is who they claim to be, but not every user is the same. After the user authenticates, the user may want to access data or services. Based on the information provided by the authentication process the application can allow or deny the user to access information or services. In other words, it needs to be evaluated if the user is authorized to access data from a service. Furthermore, authorization offers granular control to distinguish between read, write or execute access to individual resources, typically access control lists (ACL) are used for each operation [cf. (\cite{Todorov:2007:MUI}), (\cite{Boyed:2012:GSOA})].

Systems offer a user login process or sign-in process, in order to receive the information necessary to make authorization decisions. The login process initiates the authentication process between the user and the system. As a result of this process where the user has to prove their identity, the user receives a system or application specific structure. In a federated identity system, this is called an assertion. The assertion holds an identifier and identification information about the user which can indicate for example what kind of resources the user can access. For every action, the user then has to provide the assertion and based on the information provided within the assertion the user is then either granted or denied access. Another example of the usage of the assertion is the personalization of websites [cf. (\cite{Todorov:2007:MUI}), (\cite{NIST:2017:DIG})].

In federated systems, the verifier of authentication information of the user is called Identity Provider (IdP), and the party that receives and uses information is called the Relying Party(RP). In the context of the federated identification systems the user, or the one that is trying to access information from a system is called the subscriber. The IdP generates an assertion for the verifier associated with the subscriber. This process allows subscribers to access multiple RPs without maintaining separate credentials and the process also supports SSO [cf. \cite{NIST:2017:DIGFA}].

The paper 'Digital Identity Guidelines - Federation and Assertion' by Grassi, Richer, et al. (2017), defines different levels of federation assurance that define how assertion should be constructed and secured for a given transaction. Each successive level has to fulfill the requirements of all lower level. Federation assurance levels defined by the paper 'Digital Identity Guidelines - Federation and Assertion' by Grassi, Richer, et al.  (2017) are shown in table~\ref{tab:fal}.



\begin{table}[h]
	\centering
	\begingroup
	\setlength{\tabcolsep}{10pt} % Default value: 6pt
	\renewcommand{\arraystretch}{1.5} % 
	\begin{tabular}{cl}
		\hline
		\multicolumn{2}{c}{\cellcolor[HTML]{656565}{\color[HTML]{FFFFFF} Federation Assurance Level}}                                                                                                                                                                                                                                                                   \\ \hline
		\multicolumn{1}{|c|}{\textbf{FAL1}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}The first level requires a bearer assertion, signed with approved cryptography\\ by an IdP, for example, OpenID Connect Basic Client profile or Security As-\\ sertion Markup Language (SAML) Web SSO Artifact Binding profile with no\\ additional feature.\end{tabular}} \\ \hline
		\multicolumn{1}{|c|}{\textbf{FAL2}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}The second level requires an assertion like the OpenID Connect ID Token or\\ SAML Assertion, to be encrypted with the public key of the RP, whereas the\\ RP is the only party that can decrypt the bearer assertion.\end{tabular}}                                        \\ \hline
		\multicolumn{1}{|c|}{\textbf{FAL3}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Additionally, to the first two levels the last federation assertion level requires\\ the subscriber to prove their possession of a key that is bound to the\\ assertion (holder of key assertion) and initially was used to authenticate to\\ the IdP.\end{tabular}} \\ \hline
		
	\end{tabular}
	\endgroup
	\caption{Federated Assurance Levels (\cite{NIST:2017:DIG}, p.19)} \label{tab:fal}
\end{table}

For any assertion level, the IdP has to make sure that an RP cannot impersonate the IdP at another RP by signing the assertion. The signing of the assertion can be done by either a Media Access Control (MAC) using a shared key or a digital signature using an asymmetric key [cf. (\cite{NIST:2017:DIG}), (\cite{NIST:2017:DIGFA})].  

One of the factors that differentiate the federation assurance level is the usage of assertion binding. An assertion binding can be chosen based on the RP requirements. It may be required for example that the RP needs additional proof of the binding of an assertion to a particular subscriber. The two different kinds of assertion binding to chose from is the bearer and holder-of-key assertions. Any party can present a bearer assertion in order to prove they have the identity of the bearer. Based on this handling of bearer assertions it can be assumed, that if an attacker is successful in capturing a bearer assertion, the attacker can represent the subscriber that was previously associated with this bearer assertion. The attacker could present the assertion or reference to the RP and impersonate the subscriber. The holder-of-key assertion in comparison holds a reference to a key which indicates which subscriber is representing the assertion. The key is signed and asserted by the issuer of the assertion [cf. (\cite{NIST:2017:DIGFA})].

Typically tokens are used for assertions, in this context, tokens are software tokens which are used to provide access control for systems. The assertion binding described before can also be referred to as token profiles. Typically there are two kind of tokens that are frequently used, which is the access token and the refresh token. An access token, when it is valid can be used to access resources. If the access token is expired and not valid anymore a refresh token can be used to reclaim a new access token. OpenID Connect additionally defines an ID token which will be discussed later [cf. (\cite{Spencer:2018:APISecurity})]. 

Furthermore, tokens can be passed either by reference or by value. On the one hand, if tokens are passed by reference, they have to be resolved in another instance. On the other hand, a token can be passed by value and already has all the necessary information about the user to establish for example a session. Those two ways of passing a token are significant when it comes to security considerations. Also, tokens have different types. The OAuth specification, for example, does not specify which kind of token has to be used. Spencer (2018), lists some of the typical tokens: 

\begin{itemize}
	\item WS-Security token (SAML)
	\item JSON Web Tokens (JWT)
	\item Legacy tokens
	\item Custom tokens
\end{itemize}

In praxis developers often use JWT's because they are very flexible and allow to add additional claims. Also, they use JavaScript Object Notation (JSON) and not Extensible Markup Language (XML) like SAML tokens, which is much easier to process. More about JSON Tokens can be read in the upcoming chapter Token-based Authentication [cf. (\cite{Spencer:2018:APISecurity})].

\section{Token-based Authentication}
\label{tokenBasedAuthentication}

The way web developers write back-end applications has changed significantly with the rising popularity of single page and mobile applications. Backend-developers no longer spend a lot of time building a markup. Instead, they build APIs for frontend applications to consume. The split up of frontend and backend allows the backend to focus on business logic and data management while the frontend solely focus on the representation of the content. The number one way single page and mobile web applications are authenticating users according to Tkalec (2015) is token-based authentication [cf. (\cite{Tkalec:2015})].  

Serilleja (2015) shares the view that token-based authentication is the modern way to handle authentication. Token-based authentication should be considered because of various factors. It is optimal for mobile applications which work in a stateless way and need to adapt to the sudden change of demand in a scalable way. Furthermore, token-based authentication provides extra security and applications can pass on authenticated users to other applications. Serilleja (2015) and Tkalec (2015) concluded that token-based authentication is the best alternative for modern web applications to authenticate their users. Serilleja (2015) examines how authentication was done in the past to show difficulties and limits past approaches had and highlight the advantages of token-based authentication. One approach to authenticate users, used in the past is server-based authentication [cf. (\cite{Serilleja:2015:Scothio}), (\cite{Tkalec:2015})]. 

\subsection{Server-Based Authentication.}
A lot of modern-day API’s built on the Representational State Transfer (REST) programming paradigm which basis is the Hypertext Transfer Protocol (HTTP) which is stateless as well. A protocol that is stateless does not recall the actions that were taken beforehand which mean for example that if we authenticate the user, in the next request we have to authenticate the user again because the application will not know the user anymore [cf. (\cite{Serilleja:2015:Scothio})]. 

The aim of server-based authentication is for the application to remember the user that logged on at the application. The application has to store the information on the server, which can be done in a few different ways during the session, usually in memory on the disk. The workflow of a server-based authentication starts with the server delivering the website and the user logging in with username and password. The server saves the information from the user login info in a session. After the session is established, the session is checked on the server for every request. If the session is valid, the server returns the requested data to the client. However, since modern Single Page Applications (SPA) and mobile applications are on the rise, this method to authenticate users shown some problems, especially when it comes to scalability [cf. (\cite{Serilleja:2015:Scothio})]. 

\pagebreak[4]

The session handling with server-based authentication is especially hard on the server’s bandwidth. Most of the time the session gets established in memory on the server when the user authenticates. This approach leads to an enormous overhead. The second problem with this approach is that the information of the user is also held in memory on the server. Since more and more companies are moving servers to the cloud, this is not only a security issue, also replicating servers for scale is limited. Also nowadays users want to access their data at any moment from every device. Providing the user with the possibility to access data across multiple mobile devices is vital, which means cross-origin resource sharing has to be enabled. With server-based authentication, it is possible to run into problems with cross-origin request [cf. (\cite{Serilleja:2015:Scothio})].


\subsection{
	Token-based authentication with JSON Web Token
}

The most important thing about token-based authentication is that it is stateless, much like HTTP. The server does not have to hold the session of the user over an extended period on the server. Instead, the user can request resources by offering a token generated by an authentication server. The token sent in the query string or authorization header, can then be validated at the resource server and the secure resource will be returned to the user. The approach of using JSON Web Tokens (JWT) provides the ability to scale applications without considering on which domain the user logged in. Another advantage besides being scalable is that token-based authentication gives one the possibility to reuse the same token for authenticating the user. Therefore, it gets easier to build applications that share permission with other application because many separate servers, running on multiple platforms and domains can reuse the same token. The approach also gives performance advantages compared to server-side authentication because there is no need to find and deserialize the session on each request. However, since it is best practice to encrypt the token, the token still needs to be validated, and the content needs to be parsed. One way to implement token-based authentication is with the help of JWT's. JWT's are gaining popularity fast and are backed by huge companies like Google and Microsoft. Also, the Internet Engineering Task Force defines a standard specification for JSON Web Tokens. OpenID and OAuth also use JWT as a standard; therefore, the usage of the JWT will be explained in detail [cf. (\cite{Tkalec:2015})].

\pagebreak[4]

JWT is a compact structure that holds information about the authentication of a user or claims. The structure is indented for space-constrained environments such as HTTP Authorization headers. The payload of JWT is of JSON, enabling the claims to be digitally signed or integrity protected with a MAC and encrypted [cf. (\cite{JWT:IETF:Jones:2015})].

The standard is used to transport data between interested parties. The transferred data can be for example the identity of a user or user’s entitlements. Furthermore, with the possibility of digital signatures and encryption, data can be transferred securely over an unsecured channel. The signatures also allow asserting the identity of a user if the recipient trusts JWT is asserting party [cf. (\cite{Siriwardena:JWTJWSJWE:2016})].

An example of a JWT is an id\_token. Google provides an OAuth 2.0 Playground for Developers, where scopes can be chosen and tried out against the Google API. To get an excellent example of a JWT, OAuth2 API v2 was selected and authorized like shown in figure~\ref{fig:googleoauthplaygroundoauthapi} [cf. (\cite{Google:2018:OAuthPlayground})].


\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/googleOAuthPlaygroundOAuthAPI}
	\caption[OAuth API]{Google Developers Playground OAuth 2.0 API}
	\label{fig:googleoauthplaygroundoauthapi}
\end{figure}

After choosing and authorizing with a Google Account, the Google API returns an Authorization Code. This Authorization Code is specific for a certain Authentication Flow defined by OAuth and can then be exchanged for the tokens. As a result, all information for the scopes that were selected beforehand is returned. Because OAuth 2 API v2 was selected, the returned value is an id\_token. This id\_token is a nice representation of a JWT and holds the standard information that is required according to the OAuth specification  [cf. (\cite{Google:2018:OAuthPlayground})].

\pagebreak[4]

\lstset{caption=JWT Token,breaklines=true, breakatwhitespace=true, basicstyle=\small\ttfamily, label={lst:jwt}, tabsize=3, basewidth={0.55em}, fontadjust, language=XML}
\begin{lstlisting}
eyJhbGciOiJSUzI1NiIsImtpZCI6ImRhZDQ0NzM5NTc2NDg1ZWMzMGQyMjg4NDJlNzNh
Y2UwYmMzNjdiYzQifQ
.
eyJhenAiOiI0MDc0MDg3MTgxOTIuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJh
dWQiOiI0MDc0MDg3MTgxOTIuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIi
OiIxMTIzMDE5MzgzMjI4MTAyMzk3MTIiLCJlbWFpbCI6ImNvcm5lbGlhcmF1Y2hAZ214
LmF0IiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImF0X2hhc2giOiJSdjJRQjZoZzFqdDZR
aHRJWG5laWVnIiwiZXhwIjoxNTI5NzQ3MTk2LCJpc3MiOiJodHRwczovL2FjY291bnRz
Lmdvb2dsZS5jb20iLCJpYXQiOjE1Mjk3NDM1OTYsIm5hbWUiOiJDb25ueSBSYXVjaCIs
InBpY3R1cmUiOiJodHRwczovL2xoNC5nb29nbGV1c2VyY29udGVudC5jb20vLWJFbEt3
aDVaNUJnL0FBQUFBQUFBQUFJL0FBQUFBQUFBQk5rL0p5Qm1XTW9uYUlJL3M5Ni1jL3Bo
b3RvLmpwZyIsImdpdmVuX25hbWUiOiJDb25ueSIsImZhbWlseV9uYW1lIjoiUmF1Y2gi
LCJsb2NhbGUiOiJkZSJ9
.
yibOtrXy9_-cfOWytzwGuE4zqLv-MK_2-PYIKR_xecJt9ACnMnNMSmio6i8Vu7U061wF
OTb-qRennHbvy3lTRZLcTXttIrIUl-NdnZs2BrTSGWrw9aRzEjIHAXiY4fGRHj9VZXs_
_J3Nn0EoBmT7Cnua2hb4U_X3hAyGpEvlSGKc5HvbyzOAtNh081Cyj1TI-AidCPTuC5vh
68C55tLJ87PWNm8WU1rCPOPBdVTjhYlqJKCpgUJ39_p_MXL_uHBZXRrvbOyV_tZVlw47
rjd8GFnBq1QqsYAR-6wrFbNL1pY6tPyriqZnQdi5KqYWPUwGpXbFDUfhZAmWXT8-PTsc
gQETEp8o3RvRHtfSu8Gx4UOhukt9_VxVdHmpFw
\end{lstlisting}

\goodbreak

The JWT Token in the listing~\ref{lst:jwt} is presented as a sequence of URL-save base64url-encoded values. The different values are separated by a (‘.’) character. How many parts a JWT has is dependent on how the JWT is serialized. Either by using the JWS Compact Serialization or JWE Compact Serialization [cf. (\cite{JWT:IETF:Jones:2015})].


To understand this JWT token in the listing~\ref{lst:jwt} it is best to look at each of the three parts of the JWT separately. When decoding the first part of the JWT, we receive a JSON object. Each part can be decoded individually but if a quick representation of a token is needed developers are the best advised to use \href{https://jwt.io/} {https://jwt.io/}. The website not only decodes the information of the token it also verifies the token. The website shows a warning if the token is not flawless according to the JWT specification. The verification of the token and the signing is done with multiple libraries that provide information about certain vulnerabilities of the JSON Web Token. The first decoded part of the JWT gives us the following JSON [cf. (\cite{JWT:IETF:Jones:2015})].

\pagebreak[4]

\lstset{
	caption=JOSE Header,
	label={lst:joseheader},
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black}
}
\begin{lstlisting}
{
"alg": "HS256",
"typ": "JWT"
}
\end{lstlisting}


The JSON object in listing~\ref{lst:joseheader} is the JOSE Header, representing the type of the token, the cryptographic operations applied and optionally additional properties of the JWT. Based on the information of the JOSE Header it can explain if the JWT is a JWS or a JWE. When speaking of JWT, we speak of one of the implementations of JWT because in fact, JWT does not exist itself. Concrete implementation of the JWT is JSON Web Signature (JWS) or JSON Web Encryption (JWE). The figure~\ref{fig:jwtjwsjwe2} gives an optical representation of the structure [cf. (\cite{Siriwardena:JWTJWSJWE:2016})].


The 'type' Header Parameter we can examine in listing~\ref{lst:joseheader} indicates the kind of token, considering the example it is a JSON Web Token. The second parameter is the 'alg' Header Parameter and is specified in either the 'JSON Web Security (JWS)' reference documentation written by Jones, Bradley \& N. (2015) or the 'JSON Web Encryption (JWE)' reference documentation written by Jones \& Hilbrand (2015). In this case, the JWT is a JWS since a JWE needs more specific Header Parameter. In the JWS the 'alg' Header Parameter gives information about the algorithm which was used to create the signature. Which kind of arguments are accepted by the 'alg' Header Parameter are explained in yet another specification called 'JSON Web Signature and Encryption Algorithms' written by Jones (2015). In the listing~\ref{lst:joseheader} the 'HS256' algorithm is used, meaning that the JWS was MACed using the HMAC SHA-256 algorithm. The definition of the 'alg' Header Parameter in the 'JSON Web Signature (JWS)' reference documentation is very similar to the definition in the 'JSON Web Encryption (JWE)' reference documentation by Jones \& Hilbrand (2015) except the cryptographic algorithm is used to encrypt or determine the value of the Content Encryption Key (CEK). Jones \& Hilbrand (2015), also defines an 'enc' Encryption Algorithm Header, which is used for content encryption on plaintext. The authenticated encryption performed produces a ciphertext and the authentication tag. Also for the 'enc' Header Parameter, the valid arguments can be found in the 'JSON Web Signature and Encryption Algorithms' written by Jones (2015). The algorithm used must be an AEAD algorithm with a specified key length. There is further Header Parameter that can be defined, here are just the essential JOSE Header Parameter for this document listed [cf. (\cite{JWE:IETF:Jones:2015}),( \cite{JWA:Jones:2015}), (\cite{JWS:IETF:Jones:2015})]. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{images/jwtjwsjwe2}
	\caption{JWT Inheritance (\cite{Siriwardena:JWTJWSJWE:2016})}
	\label{fig:jwtjwsjwe2}
\end{figure}

\pagebreak[4]

The second decoded part of the example in listing ~\ref{lst:jwt}, represents the claim set. The representation of the claim set is a JSON, where each key has to be unique. If the keys are duplicated a JSON parsing can occur or the last one of the duplicated keys is returned. 


\lstset{
	caption=Claim Set,
	label={lst:claimset},
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black}
}
\begin{lstlisting}
{
"azp": "407408718192.apps.googleusercontent.com",
"aud": "407408718192.apps.googleusercontent.com",
"sub": "112301938322810239712",
"email": "corneliarauch@gmx.at",
"email_verified": true,
"at_hash": "Rv2QB6hg1jt6QhtIXneieg",
"exp": 1529747196,
"iss": "https://accounts.google.com",
"iat": 1529743596,
"name": "Conny Rauch",
"picture": "https://lh4.googleusercontent.com/-bElKwh5Z5Bg/
AAAAAAAAAAI/AAAAAAAABNk/JyBmWMonaII/s96-c/photo.jpg",
"given_name": "Conny",
"family_name": "Rauch",
"locale": "de"
}
\end{lstlisting}

The decoded value from the example in listing~\ref{lst:jwt} of the Google OAuth 2.0 Playground  by Google (2018), returns a JSON object shown in the listing \ref{lst:claimset}. The claim set we receive is composed of mandatory and optional claims. Specifically requested in this example were the login, email and profile scope which affected the claims returned from the Google OAuth 2.0 API. Mandatory claims for the login with OpenID are iss, iat, aud, sub and exp and an optional claim that was returned from the API is azp. For the profile scope, the specific claims that were returned are name, picture, given\_name, family\_name, and locale. Email specific claims are email and email\_verified. Furthermore, identity providers can include additional elements that are neither mandatory or optional claims [cf. ({\cite{Google:2018:OAuthPlayground}), (\cite{Siriwardena:JWTJWSJWE:2016})]}].

The last and third part from the example in listing \ref{lst:jwt}  represents the base64url-encoded signature. To know which kind of signature the decoded code is resembling a peek at the JOSE Header will help. The JOSE Header as mentioned before gives information about the cryptographic elements that were used related to the signature. In case of listing~\ref{lst:signaturejwt} the Google OAuth 2.0 API uses RASSSA-PKCS1-V1\_5 with the SHA-256 hashing algorithm. However, the \href{https://jwt.io/} {https://jwt.io/} website does not provide us with the decoded public key. 


\lstset{
	caption=Signature JWT,
	label={lst:signaturejwt},
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black}
}
\begin{lstlisting}
{
RSASHA256(
base64UrlEncode(header) + "." +
base64UrlEncode(payload),
Public Key or Certificate.
)
}
\end{lstlisting}


To serialize an encrypted message, one has to follow either the JWS or the JWE specification. Each of the specifications has the type’s compact serialization. Google OpenID Connect uses the compact serialization. The OpenID Connect specification suggest using the JWS compact serialization or the JWE compact serialization. In this paper only the compact serialization will be discussed, the specification of the other serialization method can be either looked up in the JWE specification or the JWS specification. To call a JWS or a JWE, a JWT has to follow the compact serialization [cf. (\cite{JWS:IETF:Jones:2015}), ({\cite{JWE:IETF:Jones:2015})].
	
	The aim of the JWS Compact Serialization is it to present content as a compact, URL-safe string, which is either digitally signed or MACed. It is not possible to use multiple signature or MAC in a JWS Compact Serialization and furthermore it is not allowed to use JWS Unprotected Headers. An unprotected header is a JSON object which includes the header element that is not integrity protected, which concludes that a protected header is a JSON object that is integrity protected by using MAC or digital signatures. A JWS Compact Serializations is represented as a concatenated string shown in listing~\ref{lst:jwscompactserialization} [cf. (\cite{JWS:IETF:Jones:2015})].
	
	\lstset{
		caption=JWS Compact Serialization,
		label={lst:jwscompactserialization},
		string=[s]{"}{"},
		stringstyle=\color{blue},
		comment=[l]{:},
		commentstyle=\color{black}
	}
	\begin{lstlisting}
	{
	BASE64URL(UTF8(JWS Protected Header)) '.'
	BASE64URL(JWS Payload) '.'
	BASE64URL(JWS Signature)
	}
	\end{lstlisting}
	
	The first element is called the JOSE header which contains all the information that advertises the public key corresponding to the private key that was used to sign the message. Elements of the header include the jku, jwk, kid, x5u, x5c, x5t and x5t\#s256. The second element is the JWS Payload or the content to be signed, which does not have to be JSON. The following approach is used  to construct the message: ASCII(BASE64URL-ENCODE(UTF8(JOSE Header)) '.' BASE64URL-ENCODE(JWS Payload)). 
	The last element is the JWS signature, which is computed over the message that was constructed beforehand, using the algorithm defined in the JOSE Header. These three key components together are called a JWS token. When using the JWE Compact Serialization, the output is called JWE token. Compared to the JWS token, which we saw above the JWS token consists of 5 different key components [cf. (\cite{JWS:IETF:Jones:2015}), (\cite{JWE:IETF:Jones:2015})].
	
	
	\lstset{
		caption=JWE Compact Serialization,
		label={lst:jwecompactserialization},
		string=[s]{"}{"},
		stringstyle=\color{blue},
		comment=[l]{:},
		commentstyle=\color{black}
	}
	\begin{lstlisting}
	{
	BASE64URL-ENCODE(UTF8(JWE Protected Header)) '.'
	BASE64URL-ENCODE(JWE Encrypted Key) '.'
	BASE64URL-ENCODE(JWE Initialization Vector) '.'
	BASE64URL-ENCODE(JWE Ciphertext) '.'
	BASE64URL-ENCODE(JWE Authentication Tag)
	}
	\end{lstlisting}
	
	Both digital signatures and MACs can be used to provide integrity checking. However, specification warns that there are significant differences that have to be considered when designing a protocol. MACs only provide the origination of the identification under specific circumstances. It is normally assumed that the private key used for the signature is only known by a single entity. Although in the case of MAC keys all the entities that use it for integrity computation need to know the MAC key in order to validate the message. That means that with MAC validation one can tell if a message is generated from one of the entities that know the symmetric MAC key and not where the message originated [cf. (\cite{JWT:IETF:Jones:2015})].	

	
\section{Single Sign-On Federation Systems}

Application security is a complex task and developing a customized siloed identity solution can be expensive. A Stand-alone identity store can besides being expensive also causes information assurance and administrative problems for organizations  [cf.(\cite{JerichoSystems:IS})]. 

Boyed (2012) says that a federate authentication or identity federation, is a system that is maintaining its accounts, for example, username and password databases, with the help of third-party service. Often big corporate IT environments already use such solutions. Environment applications, for example, may trust an Active Directory server, an LDAP server or a SAML provider [cf. (\cite{Birell:2013:FIMS}), {\cite{Boyed:2012:GSOA}].
	
		\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{images/federation}
		\caption[Federation]{Federation (\cite{NIST:2017:DIGFA})}
		\label{fig:federation}
	\end{figure}
	
	
	As shown in figure~\ref{fig:federation} an identity system usually consists of three parties. The user or subscriber, service provider or resource provider, and the identity provider. Depending on the protocol which is used, different information passes between the three components at different times. While the user or subscriber communicates with the IdP and the RP over a browser the IdP and the RP can communicate over a front channel which works through redirects involving the subscriber or a back channel which is a direct connection not including the subscriber [cf. (\cite{NIST:2017:DIGFA})]. 

	
	
	The user or subscriber is usually associated with a real user, and their identity is represented by a set of attributes. A single user can be associated with multiple identities. A service/resource provider is responsible for making an authorization decision based on authentication assertion and the attributes derived from it. Many of the service providers implement their identity management, but in federated systems, the identity management is outsourced to the identity provider. An identity provider can be either a stand-alone service or be a part of the service provider. The responsibility of the IdP is it to authenticate the user, issuing authentication assertions, manage identities whereas the IdP has the right to create, update, release and delete attributes associated with the identity [cf. (\cite{Birell:2013:FIMS})]. 
	
	
	The 'Digital Identity Guidelines' by Grassi, Garcia \& L. (2017) claim that identity federation is preferred over some siloed identity solution that each serve a single agency or Relying Party (RPs). Furthermore The 'Digital Identity Guidelines' by Grassi, Garcia \& L. (2017), lists specific benefits that come with using federated architectures, as can be examined before. 
	
	\begin{itemize}
		\item Enhanced user experience. For example, an individual can be identity proofed once and reuse the issued credential at multiple RPs. 
		\item Cost reduction to both the user (reduction in authenticators) and the agency (reduction in information technology infrastructure). 
		\item Data minimization as agencies do not need to pay for collection, storage, disposal, and compliance activities related to storing personal information. 
		\item Pseudonymous attribute assertions as agencies can request a minimized set of attributes to include claims or to fulfill service delivery. 
		\item Mission enablement as agencies can focus on the mission, rather than the business of identity management.
	\end{itemize}
	
	Essentially switching to a federated identity solution should help to reduce the management burden that comes from managing multiple accounts and can lead to potential points of failure.  Furthermore, users are giving control over their attributes dissemination which leads to privacy violations and identity theft. Most of the existing identity federation solution focuses on the individual system, each of which focuses on one of three general types of functionality - SSO, federated identity or anonymous credentials. The SSO system can issue authentication assertion to multiple service providers. Examples for SSO are Passport, OpenID or Shibboleth. Federated identity systems are focused on managing multiple identities for a single user. Examples for federated identity are Project Liberty (http://projectliberty.org), Higgins (www.eclipse.org/higgins), PRIME (www.prime-project.eu), CardSpace, and
	Client-Side Federation. Anonymous credentials systems provide authentication assertions while not revealing the user's identity to a service provider. Examples include Idemix, U-Prove, and P-IMS [cf. (\cite{Birell:2013:FIMS}), (\cite{Boyed:2012:GSOA}), (\cite{NIST:2017:DIG})].
		
		\subsection{Single Sign-On}
		\label{singlesignon}	The focus of this thesis is on mobile SSO solutions. SSO aims to design an authentication system that serves the interests of the user as well as the interests of the service provider. Whereas the user prefers a simple process, the service provider requires a complicated authentication procedure. Ironically trying to make the authentication procedure more save often leads to weakening the whole system due to the user always finding new ways to bypass it. An example mentioned before for this conundrum, is password fatigue. Another challenge that SSO is trying to tackle is that standard web authentication solutions that require the user to login with a password, only authenticate the user and are not capable of providing access control or revealing additional information about the user. Most SSO solutions, therefore, try to combine the authentication process and authorization [cf. (\cite{Prochazka:2010:UCA})].
		
		The paper 'Taxonomy of Single Sign-On System' by Pashalidis \& Mitchell (2003) identifies four generic architectures for SSO systems. An SSO system has to authenticate a user to an Service Provider (SP). Because authentication also implies identification, SSO systems have to incorporate the lifecycle management of identifiers that can take various forms. The paper distinguishes between two main types of SSO systems. The first type is ‘pseudo-SSO’ and the second type is ‘true SSO’. Typical pseudo-SSO are providing automatic authentication for all SP specific authentication methods after the user initially authenticated with the pseudo-SSO component, which is called primary authentication. The responsibility of a pseudo-SSO service is to manage all identities. A user may have multiple SSO identities for a single SP, but in principle, one SSO identity corresponds to one SP. Compared to the pseudo-SSO system in a true-SSO system, the user has to initially authenticate with the Authentication Service Provider (ASP) that is required to have an established relationship with all SPs. The relationship between the ASP and the SP has to be trustworthy. Key functionality of the true-SSO service is that the only authentication that includes the user occurs between the user and the ASP. The SP will get notified of the authentication status, including information about the identity of the user, with so-called authentication assertions. The categorization of SSO architectures can be further distinguished by the location of the ASP/pseudo-SSO component. This component can be either local to the user platform or offered as a third-party service also called SSO-proxy. This further categorization leads Pashalidis \& Mitchell (2003) to the four generic architectures mentioned above Local pseudo-SSO systems, Proxy-based pseudo-SSO systems, Local true SSO systems and Proxy-based true SSO systems  [cf. (\cite{Pashalidis:2003:10.1007/3-540-45067-X_22})].
		
		When opting for a particular SSO architecture, one has to consider the strength and weaknesses of each system carefully. The paper, therefore, analyses the four generic architectures regarding pseudonymity and unlinkability, anonymous network access, support for user mobility, deployment costs, maintenance costs, running cost and trust relationships. Pseudonymity and unlinkability refer to the fact that the user providers sensible information that needs to be protected and it should not be possible to correlate distinct identities of the same user and personal information, and potential properties should not be linked to the user. The unlinkability cannot be guaranteed for pseudo-SSO because the identities for the SSO are SP specific. To improve unlinkability   Pashalidis \& Mitchell (2003), suggest to use an ‘anonymizing proxy’. For local SSO system additionally services are needed to implement a anonymizing proxy. In proxy-based SSO systems the anonymizing proxy can be integrated. User mobility is supported for proxy-based SSO, for locale SSO there need to be further services in place. The deployment cost is lower for pseudo-SSO systems than for true-SSO systems. However, the maintenance cost is higher because if any SPs change the whole logic of the pseudo-SSO system has to change. The running cost of pseudo-SSO is likely to be lower than for true-SSO systems. For the pseudo-SSO systems, the trust relationship between users and SP may be dynamically changing depending on the implementation, for true-SSO the trust relationship is established between ASP and SPs and is always consistent. Generally speaking are pseudo-SSO systems are more suitable for a closed system where identity management is just for managing the life cycle of remaining credentials. For an open system, it is not enough to maintain credentials. Appropriate privacy protection services and privacy-aware identity management schemes should be therefore integrated with true SSO schemes [cf. (\cite{Pashalidis:2003:10.1007/3-540-45067-X_22})]. 
			
		Therefore two very popular assertion technologies are discussed: Shibboleth (SAML) and OpenID Connect. Those technologies are not the only possible SSO assertion technologies however, they represent the most commonly used federated identity systems. According to Lynch (2011), two SSO solutions gained broad acceptance. The two SSO solutions are SAML-based federations using SOAP with focus on large enterprises, governments and educational networks and the Web Authorization Protocol is a combination of the Protocols OpenID and OAuth. SAML federations have been customized to address the security concerns of those institutions that typically have a large user base, significantly protected resources, complex authorization patterns and data and services spread across multiple domains. However, in a Web 2.0 world, the SAML solutions where seen as too rigid and too severe to maintain; a lightweight SSO was needed, therefore the Web Authorization Protocol was designed. The approach of this protocol is taking advantage of the lightweight RESTful APIs which are reusing the existing HTTP architecture features and the JavaScript Object Notation (JSON) [cf. (\cite{Lynch:2017:IIG})].
			
		\paragraph{SAML}
		\label{SAML}
		One of the well-known solutions based on Security Assertion Markup Language version 2 (SAML2) is Shibboleth. Shibboleth is one of the leading middlewares for building identity federations in a higher education sphere. It offers authentication, authorization and attribute assertion between entities. Procházka, Kouřil \& Matyska (2010), identifies the following entities Identity Provider (IdP), Service Provider (SP), Discovery Service, Metadata Operator and Federation Operator  [cf. (\cite{Prochazka:2010:UCA})].
		
		The discovery service is used to find the users organization IdP. The IdP defines an attribute release policy and releases different sets of the user's attributes to different SPs. Users are only able to agree or disagree with the whole set of attributes because the decision comes from the IdP. When the user tries to log on, to a service of a Service Provider, the user gets redirected to a page where an IdP previously found by the Discovery Service can be chosen. If an SP wants to provide their service to multiple federations it has to negotiate policy and technical detail with each federation operator. The federation operator manages the federation policies and introduces SPs and IdPs. Furthermore, the federations operator has to maintain and manage the information about all entities in a federation which is contained in the Metadata. A problem with this architecture is that the SP has to keep track of changes of the technical specification of various federation operators to maintain the configuration for each federation operator. A solution would be a significant federation registration, but this is indeed not possible because of technical and administrative severity and political will. This solution is also somewhat misleading for users since they have to maintain multiple credentials, select from multiple identifiers and so on. According to Procházka, Kouřil \& Matyska (2010) Shibboleth is too restrictive, a solution with a centrally managed point of IdPs and SPs is preferred. Also, users should not have to deal with redundant accounts [cf. (\cite{Prochazka:2010:UCA})].		
		
		\paragraph{OAuth and OpenID Connect}
		\label{OAuthAndOpenID}
		The second solution that is introduced is OAuth 2.0 with OpenID Connect.	OAuth 2.0 is a protocol that is used for access control. OAuth 2.0 is the newest version of  OAuth. Compared to the previous version OAuth 1.0a, HTTPS is used for all request, and the complicated signature process of tokens was replaced. Additionally, complexity was reduced, and the usage is much simpler than in the first version. The new version OAuth 2.0 is not compatible with older versions [cf.  (\cite{LeBlanc:2011:SocialApplications})].
		
		The protocol adds an authorization layer – separating the Client from the Resource Owner. 
		The Client is not bound to any particular implementation characteristics and can make requests to protected resources on behalf of the resource owner. The OAuth 2.0 specification distinguishes between two Client Types, Confidential Clients, and Public Clients. Confidential clients are capable of maintaining the confidentiality and public clients are incapable of maintaining confidentiality [cf. (\cite{Hardt:2012:OAuth2}, p.6-14)].
		
		The Resource Owner is a role according to Sakimura et al. (2014) which can grant access to a protected resource also referred to as end-user, if the resource owner is a person. Furthermore, scopes are used to specify to which resources the resource owner wants to grant access. Requests which are sent to an authorization endpoint include scopes that can be mandatory or optional parameters. Declaring the scopes is up to the provider. An access token is used to gain access instead of the resource owner’s credentials [cf. (\cite{Sakimura:2014:OpenIDConnect}, p.6)]. 
		
			\begin{figure}[h]
			\centering
			\includegraphics[width=0.8\linewidth]{images/oaut-process2}
			\caption{OAuth Authorization Process (LeBlanc, cited in Rauch(2016))}
			\label{fig:oaut-process2}
		\end{figure}
		
		Access Tokens are issued by an Authorization Server and can be used by the Client to gain access to protected resources hosted by the resource server. The Authorization Server is responsible for issuing the access token and may be the same as the Resource Server. In literature, different terms are used do describe what an identity server does. Among these are Security Token Service, Identity Provider, Authorization Server and IP-STS. OAuth 2.0 uses a token service, which centralizes this logic and reduces complexity for the Client and the APIs [cf. (\cite{Sakimura:2014:OpenIDConnect}, p. 6), (\cite{Brock:2018:ID4}), (\cite{Boyd:2012:GSOAuth})]
		
		A typical request of a Client to access a resource on the Resource Server includes some requests. In the end, the Client should be able to access the protected resource. The process is shown in figure \ref{fig:oaut-process2}.
			
		In the first two steps, the Client redirects the user to the Resource Owner, to give the Client permission to access the protected resources. The user is then redirected to the callback location which is included in the original request and adds a verification code which confirms that the Client is entitled to access the requested resource. After the redirection to the callback location, the Client sends a request with the verification code to the Authorization Server. If the request is a success, the Authorization Server replies in step four with an access token and eventually a refresh token. With the received access token the client can now make signed requests to the resource server and receive in return protected resources as shown in step five and six inf figure~\ref{fig:oaut-process2} [cf. (\cite{LeBlanc:2011:SocialApplications}, p. 353)]
		
		
		OpenID Connect and OAuth 2.0 seem very similar but OpenID Connect is a framework built on top of OAuth 2.0. Together they focus on authentication and access control in a way that is suitable for modern applications. OpenID Connect can verify the identity of an End-User and obtain basic profile information about the user. It uses Claims to communicate information to the end-user [cf. (\cite{Sakimura:2014:OpenIDConnect})]
		
		OpenID Connect performs authentication and determines if the user is already logged on or to log on the user. The result is an authentication token which is returned to the user. A precondition of OpenID Connect is that the client, as well as the user, have to trust the IdP. The functions of the IdP are the same as in OAuth. The reason for that is that the newest version of OpenID is implemented on top of OAuth and extends the authentication process of OAuth  [cf. (\cite{Sakimura:2014:OpenIDConnect}), (\cite{Boyd:2012:GSOAuth}, p. 51)]
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.8\linewidth]{images/openid-process}
			\caption{OAuth 2.0 and OpenID Connect (Lodderstedt, cited in Rauch(2016))}
			\label{fig:openid-process}
		\end{figure}
		
		
		In figure~\ref{fig:openid-process} the added element of the OpenID Connect standard is marked with red. In order to implement the OpenID Connect Standard, the Client has to request the scope 'openid'. After the request, the progress is similar to the transitional OAuth authentication process, but when it comes to returning of the access token, an additional ID Token is returned. This ID Token holds information about the user and the authentication. The only purpose of the ID Token is to log in the client. The only thing left after this is for the client to verify the ID Token [cf. (\cite{Lodderstedt:2014:OpenID})]. 
		
		The ID Token is specified by Sakimura et al. (2014) as a JWT compared to OAuth were the type of token is irrelevant. The token is signed and has some particular claims. Further individual claims could also be added to the ID token. The ID token has claims that are required for the authentication process. Also, standard claims can be added which are predefined by OpenID Connect. Those claims could also be requested from UserInfo Endpoint shown as an additional endpoint in the figure~\ref{fig:openid-process}. The user info endpoint offers an API where additional user info can be requested [cf. (\cite{Sakimura:2014:OpenIDConnect})].
		
		Lodderstedt (2014), says that OpenID Connect is a contemporary advancement from protocols like SAML. The protocol considers the ongoing development of web services and mobile applications. Furthermore, the implementation of the protocol is quite easy and uses little HTTP requests with minimal cryptographic overhead to accomplish a complete login process [cf. (\cite{Lodderstedt:2014:OpenID})]. 
		
		
		
		
		
		
		Sakimura et al. (2014) defines three different way authentication can flow:
		
		
		\begin{enumerate}
			\item \textbf{Authorization Code Flow}  - The Authorization Code Flow returns an Authentification Code which can be used to be exchanged for an id\_token or an access\_token. The Authorization Code is used for Clients that can maintain a secret securely between themselves and the Authorization Server. Typically this approach is used for server-to-server communication when no interactive user is present. In this thesis, the focus is on user interaction [cf. (\cite{Sakimura:2014:OpenIDConnect}, p. 10)]
			
			To use the Authorization Code Flow according to Sakimura et al. (2014) the following request\_type indicates a Authorization Code Flow:
			\begin{itemize}
				\item	code: The response includes an Authorization Code. 
			\end{itemize}
			
			\item \textbf{Implicit Code Flow} - In the Implicit Code Flow the access token and the id token are directly returned to the Client. This behavior may expose information to the end-user or anyone who has access to the end-user’s Agent. Tokens are only returned from the Token Endpoint; the Authorization Endpoints is not used. Unlike the Authorization Code Flow where the separate request is used for authorization and authentication, here the Client will receive the access token as a result of the authentication which also means that there is no refresh token.  This flow is used for Clients who are implemented in a browser using scripting language [cf. (\cite{Sakimura:2014:OpenIDConnect}, p. 20), (\cite{Hardt:2012:OAuth2})]
			
			According to Sakimura et al. (2014), the following request\_types indicate an Implicit Code Flow:
			\begin{itemize}
				\item	id\_token: The response must include the parameter id\_token. The response does not include an Authorization Code, Access Token, or Access Token Type. 
				\item  id\_token token: The response includes an Access Token, an Access Token Type and an id\_token.
			\end{itemize}
			
			\item \textbf{Hybrid Code Flow} - In the hybrid code flow, tokens are returned from the Authorization Endpoint, and others are returned from the Token Endpoint. The Token Server will return not only the id token but also the code. The Client then has to use the code in order to gain the access token through a back channel call which means there is an additional round trip compared to the Implicit Code Flow. Using the code to receive the access token should minimize the risk of exposing the access token [cf. (\cite{Sakimura:2014:OpenIDConnect})].
			
			To use the Hybrid Code Flow according to Sakimura et al. (2014) the following request\_types indicate a Hybrid Code Flow:
			\begin{itemize}
				\item	code id\_token: The response includes an Authorization Code and an id\_token.
				\item  code id\_token token: The response includes an Authorization Code, an Access Token, an Access Token Type and an id\_token. 
			\end{itemize} 
			
		\end{enumerate}
		
		Both OpenID Connect and SAML SSO solution involve different layers of communication and exchange credentials using HTTP redirection and JavaScript which leads to vulnerabilities. Vulnerabilities include Cross-Site Request Forgery and Cross Site Scripting. The authentication process for SSO solution includes redirects which can be exploited by attackers. As the SP redirects the User to the IdP, an attacker could intervene and send the user to a fake IdP. If the user does not recognize to swap from the real IdP to the fake IDP, the attacker ends up with the user's credentials. This problem is called a malicious IdP, also known as “phishing” and a very well-known attack against SSO. Another problem that a federation SSO system could be facing is a malicious RP or SP. After a user successfully authenticates on the IdP site, the user is redirected back to the SP with a link. An attacker obtains a link with methods like sniffing and uses the link to get logged into the SP as the User [cf. (\cite{Xu:2015:SPSSO})].
		
		OpenID Connect and SAML both provide SSO solutions. However, there is the view of the differences between the two protocols. First, in SAM an SP and OpenID, the RP redirects the user to the IdP. The assertions distinguish themselves by the markup language which is used. SAML uses a signed XML document and OpenID uses a signed JSON document. The JSON format makes tokens easier to parse with Javascript. Furthermore, OpenID Connect was designed to work with the web while SAML was readjusted to work on top of the web 	[cf. (\cite{Schwartz:2016:OAuthVsSamel}), (\cite{KeyCloack})].
		
		Schwartz (2016), suggest using OpenID Connect for mobile applications, API's and any new applications in general. SAML is just suggested for applications that already use SAML. Nevertheless, KeyCloack (2018) expresses that choosing between this SSO protocol is not straightforward. Although they recommend the use of OpenID Connect, they notice the gradual development of OpenID Connect and how more and more features are implemented that SAML had for years. Therefore some people tend to pick SAML over OIDC, based on the perception that it is more mature [cf. (\cite{Schwartz:2016:OAuthVsSamel}), (\cite{KeyCloack})].
		

\chapterend